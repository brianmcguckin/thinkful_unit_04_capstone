{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Import, inspect, & prepare data\n",
    "\n",
    "The corpus used for this project is the Reuters-21578 dataset, which is pre-loaded into nltk with category labels applied to each article (some having more than one label). This notebook entails processing the data, splitting it into a couple different classification tasks, and using some unsupervised feature generation and clustering techniques to see how well they can classify the articles they're given. Then the same data is given to some supervised classification models to see how they perform with it. Finally, a new-to-me dimensionality reduction technique called UMAP is employed and results compared to the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training/1', 'training/10', 'training/100']\n",
      "['test/14826', 'test/14828', 'test/14829']\n",
      "7769\n",
      "3019\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "\n",
    "#each file is pre-labeled for train (~73%) and test (remaining 27%)\n",
    "train_files = list(filter(lambda x: x.startswith('training'), reuters.fileids()))\n",
    "test_files = list(filter(lambda x: x.startswith('test'), reuters.fileids()))\n",
    "\n",
    "print(train_files[:3])\n",
    "print(test_files[:3])\n",
    "print(len(train_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating the drought since early\n",
      "  January and improving prospects for the coming temporao,\n",
      "  although normal humidity levels have not been restored,\n",
      "  Comissaria Smith said in its weekly review.\n",
      "      The dry period means the temporao will be late this year.\n",
      "      Arrivals for the week ended February 22 were 155,221 bags\n",
      "  of 60 kilos making a cumulative total for the season of 5.93\n",
      "  mln against 5.81 at th\n"
     ]
    }
   ],
   "source": [
    "#see what this data looks like\n",
    "train_raw = reuters.raw(train_files)\n",
    "test_raw = reuters.raw(test_files)\n",
    "\n",
    "print(train_raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text cleaning function\n",
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ', text) #double hyphens cause problems\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text) #matches characters that are not letters or numbers\n",
    "    text = re.sub(r'\\d+','', text) #this handles digits\n",
    "    \n",
    "    #comparison operators format weird\n",
    "    text = re.sub(r'&lt;', ' ', text)\n",
    "    text = re.sub(r'&gt;', ' ', text)\n",
    "    text = re.sub(r'&le;', ' ', text)\n",
    "    text = re.sub(r'&ge;', ' ', text)\n",
    "    \n",
    "    #lowercase and join\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7769 train files; data shape is (6577, 3)\n",
      "articles eliminated: 1192\n",
      "\n",
      "dataframe head: \n",
      "       category          fileid  \\\n",
      "0         cocoa      training/1   \n",
      "1           acq     training/10   \n",
      "2  money-supply    training/100   \n",
      "3           acq   training/1000   \n",
      "4          earn  training/10000   \n",
      "\n",
      "                                                text  \n",
      "0  bahia cocoa review showers continued throughou...  \n",
      "1  computer terminal systems lt cpml completes sa...  \n",
      "2  n z trading bank deposit growth rises slightly...  \n",
      "3  national amusements again ups viacom lt via bi...  \n",
      "4  rogers lt rog sees st qtr net up significantly...  \n",
      "\n",
      "3019 test files; data shape is (2583, 3)\n",
      "articles eliminated: 436\n",
      "\n",
      "dataframe head: \n",
      "  category      fileid                                               text\n",
      "0    trade  test/14826  asian exporters fear damage from u s japan rif...\n",
      "1    grain  test/14828  china daily says vermin eat pct grain stocks a...\n",
      "2     ship  test/14839  australian foreign ship ban ends but nsw ports...\n",
      "3     gold  test/14842  western mining to open new gold mine in austra...\n",
      "4      acq  test/14843  sumitomo bank aims at quick recovery from merg...\n"
     ]
    }
   ],
   "source": [
    "#populate train df\n",
    "train_df = pd.DataFrame()\n",
    "for file in train_files:\n",
    "    if len(reuters.categories(file)) == 1:\n",
    "        train_df = train_df.append({\n",
    "            'category':''.join(reuters.categories(file)),\n",
    "            'fileid':file,\n",
    "            'text':text_cleaner(reuters.raw([file]))}, ignore_index=True)\n",
    "\n",
    "#inspect results        \n",
    "print('\\n{} train files; data shape is {}'.format(\n",
    "    len(train_files), train_df.shape))\n",
    "print('articles eliminated: {}\\n'.format(len(train_files) - len(train_df)))\n",
    "print('dataframe head: \\n{}'.format(train_df.head()))\n",
    "\n",
    "#populate test df\n",
    "test_df = pd.DataFrame()\n",
    "for file in test_files:\n",
    "    if len(reuters.categories(file)) == 1:\n",
    "        test_df = test_df.append({\n",
    "            'category':''.join(reuters.categories(file)),\n",
    "            'fileid':file,\n",
    "            'text':text_cleaner(reuters.raw(file))}, ignore_index=True)\n",
    "\n",
    "#inspect results\n",
    "print('\\n{} test files; data shape is {}'.format(len(\n",
    "    test_files), test_df.shape))\n",
    "print('articles eliminated: {}\\n'.format(len(test_files) - len(test_df)))\n",
    "print('dataframe head: \\n{}'.format(test_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine dfs, will do train/test split later\n",
    "df = pd.concat([train_df, test_df], 0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories for classification\n",
    "\n",
    "Since there are 90 different categories articles are labeled with, many with only a handful of articles in each category, focus is on 2 sets of the top categories.\n",
    "- 1st set: earn & acq\n",
    "- 2nd set: crude, trade, & money-fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#narrow focus on a few top categories\n",
    "print('top categories:\\n{}'.format(df['category'].value_counts()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate classes to focus on\n",
    "df_earn = df.loc[df['category'] == 'earn']\n",
    "df_acq = df.loc[df['category'] == 'acq']\n",
    "df_crude = df.loc[df['category'] == 'crude']\n",
    "df_trade = df.loc[df['category'] == 'trade']\n",
    "df_money = df.loc[df['category'] == 'money-fx']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set data/target for classes\n",
    "X_train_earn, X_test_earn, y_train_earn, y_test_earn = train_test_split(\n",
    "    df_earn['text'], df_earn['category'], test_size=0.25)\n",
    "\n",
    "X_train_acq, X_test_acq, y_train_acq, y_test_acq = train_test_split(\n",
    "    df_acq['text'], df_acq['category'], test_size=0.25)\n",
    "\n",
    "X_train_crude, X_test_crude, y_train_crude, y_test_crude = train_test_split(\n",
    "    df_crude['text'], df_crude['category'], test_size=0.25)\n",
    "\n",
    "X_train_trade, X_test_trade, y_train_trade, y_test_trade = train_test_split(\n",
    "    df_trade['text'], df_trade['category'], test_size=0.25)\n",
    "\n",
    "X_train_money, X_test_money, y_train_money, y_test_money = train_test_split(\n",
    "    df_money['text'], df_money['category'], test_size=0.25)\n",
    "\n",
    "#earn vs acq\n",
    "X_train = pd.concat([X_train_earn, X_train_acq])\n",
    "X_test = pd.concat([X_test_earn, X_test_acq])\n",
    "y_train = pd.concat([y_train_earn, y_train_acq])\n",
    "y_test = pd.concat([y_test_earn, y_test_acq])\n",
    "\n",
    "#crude vs trade vs money\n",
    "X_train_ = pd.concat([X_train_crude, X_train_trade, X_train_money])\n",
    "X_test_ = pd.concat([X_test_crude, X_test_trade, X_test_money])\n",
    "y_train_ = pd.concat([y_train_crude, y_train_trade, y_train_money])\n",
    "y_test_ = pd.concat([y_test_crude, y_test_trade, y_test_money])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify class ratio between train & test sets\n",
    "print('earn/acq train class balance: {:0.05}'.format(\n",
    "    y_train.value_counts()[1]/y_train.value_counts().sum()))\n",
    "print('earn/acq test class balance: {:0.05}'.format(\n",
    "    y_test.value_counts()[1]/y_test.value_counts().sum()))\n",
    "\n",
    "print('trade/crude/money train class balance: {:0.05}'.format(\n",
    "    y_train_.value_counts()[1]/y_train_.value_counts().sum()))\n",
    "print('trade/crude/money test class balance: {:0.05}'.format(\n",
    "    y_test_.value_counts()[1]/y_test_.value_counts().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Feature generation using tf-idf vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=2,\n",
    "                             stop_words='english',\n",
    "                             lowercase=False,\n",
    "                             use_idf=True,\n",
    "                             norm=u'l2',\n",
    "                             smooth_idf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors for earn & acq class set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vectors\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "print('train features: {}'.format(X_train_tfidf.get_shape()[1]))\n",
    "\n",
    "#test vectors\n",
    "X_test_tfidf = vectorizer.fit_transform(X_test)\n",
    "print('test features: {}'.format(X_test_tfidf.get_shape()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors for crude, trade, and money-fx set\n",
    "- Note: to keep code readable, these variables are named the same as the earn/acq variables, with the addition of a trailing underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vectors\n",
    "X_train_tfidf_ = vectorizer.fit_transform(X_train_)\n",
    "print('train features: {}'.format(X_train_tfidf_.get_shape()[1]))\n",
    "\n",
    "#test vectors\n",
    "X_test_tfidf_ = vectorizer.fit_transform(X_test_)\n",
    "print('test features: {}'.format(X_test_tfidf_.get_shape()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: Clustering with 2 classes\n",
    "### Categories: earn & acq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train_norm = normalize(X_train_tfidf)\n",
    "X_test_norm = normalize(X_test_tfidf)\n",
    "\n",
    "X_train_svd = TruncatedSVD(2).fit_transform(X_train_norm)\n",
    "X_test_svd = TruncatedSVD(2).fit_transform(X_test_norm)\n",
    "\n",
    "n_clust = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth\n",
    "Since labels are known, ground truth can be visualized and adjusted rand index will be the primary evaluation metric (as simply looking at 2d representations of high dimensional data is not completely accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_gt = pd.Categorical(y_train).codes\n",
    "y_test_gt = pd.Categorical(y_test).codes\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:, 0], X_train_svd[:, 1], c=y_train_gt)\n",
    "plt.title('train ground truth clusters')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=y_test_gt)\n",
    "plt.title('test ground truth clusters')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means & MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import adjusted_rand_score, confusion_matrix\n",
    "\n",
    "#k-means code\n",
    "kmeans = KMeans(n_clusters=n_clust,\n",
    "                init='k-means++',\n",
    "                n_init=10)\n",
    "\n",
    "y_pred_train = kmeans.fit_predict(X_train_svd)\n",
    "y_pred_test = kmeans.predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('k-means')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:, 0], X_train_svd[:, 1], c=y_pred_train)\n",
    "plt.title('train n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_train, y_pred_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:, 0], X_test_svd[:, 1], c=y_pred_test)\n",
    "plt.title('test n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_test, y_pred_test)))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#minibatch code\n",
    "mbkmeans = MiniBatchKMeans(init='random',\n",
    "                           n_clusters=n_clust,\n",
    "                           batch_size=100).fit(X_train_svd)\n",
    "\n",
    "predict_mini_train = mbkmeans.predict(X_train_svd)\n",
    "predict_mini_test = mbkmeans.predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('MiniBatchKMeans')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:, 0], X_train_svd[:, 1], c=predict_mini_train)\n",
    "plt.title('train n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_train, predict_mini_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_mini_test)\n",
    "plt.title('test n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_test, predict_mini_test)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** both full and mini batch k-means algorithms perform better than random at assigning points to clusters, but are both still relatively far off from the ground truth clusters. On a positive note, these algorithms do tend to produce consistent clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering\n",
    "The main parameter in spectral clustering (aside from n_clusters) is the affinity parameter, which is user set similarity function for  the algorithm to use.\n",
    "- Kernels used in the project:\n",
    "    - rbf (default)\n",
    "    - sigmoid\n",
    "    - neareest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "#rbf\n",
    "sc_rbf = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='rbf').fit(X_train_svd)\n",
    "\n",
    "predict_rbf_train = sc_rbf.fit_predict(X_train_svd)\n",
    "predict_rbf_test = sc_rbf.fit_predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=rbf')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=predict_rbf_train)\n",
    "plt.title('train rbf ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_rbf_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_rbf_test)\n",
    "plt.title('test rbf ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_rbf_test)))\n",
    "plt.show()\n",
    "\n",
    "#sigmoid\n",
    "sc_sig = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='sigmoid').fit(X_train_svd)\n",
    "\n",
    "predict_sig_train = sc_sig.fit_predict(X_train_svd)\n",
    "predict_sig_test = sc_sig.fit_predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=sigmoid')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=predict_sig_train)\n",
    "plt.title('train sigmoid ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_sig_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_sig_test)\n",
    "plt.title('test sigmoid ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_sig_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 10 (default)\n",
    "sc_nn10 = SpectralClustering(n_clusters=n_clust,\n",
    "                             affinity='nearest_neighbors',\n",
    "                             n_neighbors=10).fit(X_train_svd)\n",
    "\n",
    "predict_nn10_train = sc_nn10.fit_predict(X_train_svd)\n",
    "predict_nn10_test = sc_nn10.fit_predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=nearest_neighbors')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=predict_nn10_train)\n",
    "plt.title('train nn 10 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn10_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_nn10_test)\n",
    "plt.title('test nn 10 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn10_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 25\n",
    "sc_nn25 = SpectralClustering(n_clusters=n_clust,\n",
    "                             affinity='nearest_neighbors',\n",
    "                             n_neighbors=25).fit(X_train_svd)\n",
    "\n",
    "predict_nn25_train = sc_nn25.fit_predict(X_train_svd)\n",
    "predict_nn25_test = sc_nn25.fit_predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=predict_nn25_train)\n",
    "plt.title('train nn 25 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn25_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_nn25_test)\n",
    "plt.title('test nn 25 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn25_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 5\n",
    "sc_nn5 = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='nearest_neighbors',\n",
    "                            n_neighbors=5).fit(X_train_svd)\n",
    "\n",
    "predict_nn5_train = sc_nn5.fit_predict(X_train_svd)\n",
    "predict_nn5_test = sc_nn5.fit_predict(X_test_svd)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=predict_nn5_train)\n",
    "plt.title('train nn 5 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn5_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=predict_nn5_test)\n",
    "plt.title('test nn 5 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn5_test)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** spectral clustering is very dependent on which similarity function it's using. Rbf (default) and sigmoid produce slightly better better ARI scores than k-means. Nearest neighbors shows a lot of promise in terms of agreement with ground truth clusters, but also exhibits large amounts of variance not only between different values of n_neighbors, but also between train & test sets for the same n_neighbors value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up are clustering algorithms that do not take a user defined value for number of clusters - these decide how many clusters to generate based on their own interpretation of the data\n",
    "\n",
    "### Mean-shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "bandwidth = estimate_bandwidth(X_train_svd, quantile=0.3)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "\n",
    "ms_train = ms.fit_predict(X_train_svd)\n",
    "n_clusters_train = len(np.unique(ms_train))\n",
    "\n",
    "ms_test = ms.fit_predict(X_test_svd)\n",
    "n_clusters_test = len(np.unique(ms_test))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=ms_train)\n",
    "plt.title('train quantile=0.3 (default), n_clust={}, ari={:0.5}'.format(\n",
    "    n_clusters_train, adjusted_rand_score(y_train, ms_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=ms_test)\n",
    "plt.title('test quantile=0.3, n_clust={}, ari={:0.5}'.format(\n",
    "    n_clusters_test, adjusted_rand_score(y_test, ms_test)))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#try other values for quantile parameter in bandwidth\n",
    "bandwidth2 = estimate_bandwidth(X_train_svd, quantile=0.5)\n",
    "ms2 = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "\n",
    "ms_train2 = ms2.fit_predict(X_train_svd)\n",
    "n_clusters_train2 = len(np.unique(ms_train2))\n",
    "\n",
    "ms_test2 = ms2.fit_predict(X_test_svd)\n",
    "n_clusters_test2 = len(np.unique(ms_test2))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd[:,0], X_train_svd[:,1], c=ms_train2)\n",
    "plt.title('train quantile=0.5, n_clust={}, ari={:0.5}'.format(\n",
    "    n_clusters_train2, adjusted_rand_score(y_train, ms_train2)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd[:,0], X_test_svd[:,1], c=ms_test)\n",
    "plt.title('test quantile=0.5, n_clust={}, ari={:0.5}'.format(\n",
    "    n_clusters_test2, adjusted_rand_score(y_test, ms_test2)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affinity propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from itertools import cycle\n",
    "\n",
    "af = AffinityPropagation()\n",
    "af_pred = af.fit_predict(X_train_svd)\n",
    "\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "labels = af.labels_\n",
    "\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X_train_svd[cluster_centers_indices[k]]\n",
    "    plt.plot(\n",
    "        X_train_svd[class_members, 0],\n",
    "        X_train_svd[class_members, 1],\n",
    "        col + '.')\n",
    "    plt.plot(cluster_center[0],\n",
    "             cluster_center[1],\n",
    "             'o',\n",
    "             markerfacecolor=col,\n",
    "             markeredgecolor='k')\n",
    "    for x in X_train_svd[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]],\n",
    "                 [cluster_center[1], x[1]],\n",
    "                 col)\n",
    "\n",
    "plt.title('n_clusters={}, ari={:0.5}'.format(\n",
    "    n_clusters_, adjusted_rand_score(y_train, af_pred)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Both of these algorithms perform quite poorly. While mean-shift will occasionally produce the correct amount of clusters, it's highly inconsistent and will often generate a different number of clusters upon each iteration, even within a given set of parameters. Affinity propagation not only is the slowest algorithm to run, but it also generates a ridiculous solution with thousands of clusters and ARI scores indicating it is no better than random assignment. Moving forward neither of these algorithms will be revisited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Classification of 2 classes with supervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('vanilla KNN classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(knn, X_train_tfidf, y_train, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(knn, X_test_tfidf, y_test, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('vanilla logistic regression')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(lr, X_train_tfidf, y_train, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(lr, X_test_tfidf, y_test, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier().fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('vanilla random forest classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(rfc, X_train_tfidf, y_train, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(rfc, X_test_tfidf, y_test, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgbc = xgb.XGBClassifier().fit(X_train_tfidf, y_train)\n",
    "\n",
    "print('vanilla xgboost classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(xgbc, X_train_tfidf, y_train, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(xgbc, X_test_tfidf, y_test, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** even without tuning any parameters, these classifiers all perform quite well and produce favorable results compared to the clustering algorithms. XGBoost and logistic regression are particularly effective at classifying this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Clustering with 3 classes\n",
    "### Categories: crude, trade, money-fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm_ = normalize(X_train_tfidf_)\n",
    "X_test_norm_ = normalize(X_test_tfidf_)\n",
    "\n",
    "X_train_svd_ = TruncatedSVD(2).fit_transform(X_train_norm_)\n",
    "X_test_svd_ = TruncatedSVD(2).fit_transform(X_test_norm_)\n",
    "\n",
    "n_clust_ = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_gt_ = pd.Categorical(y_train_).codes\n",
    "y_test_gt_ = pd.Categorical(y_test_).codes\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd_[:, 0], X_train_svd_[:, 1], c=y_train_gt_)\n",
    "plt.title('train ground truth clusters')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd_[:, 0], X_test_svd_[:, 1], c=y_test_gt_)\n",
    "plt.title('test ground truth clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these plots are an oversimplified representation of the ground truth clusters, these classes appear to less clearly defined in comparison to the earn/acq dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_ = KMeans(n_clusters=n_clust_,\n",
    "                 init='k-means++',\n",
    "                 n_init=10).fit(X_train_svd_)\n",
    "\n",
    "y_pred_train_ = kmeans_.predict(X_train_svd_)\n",
    "y_pred_test_ = kmeans_.predict(X_test_svd_)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd_[:, 0], X_train_svd_[:, 1], c=y_pred_train_)\n",
    "plt.title('train n_clusters=3, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_train_, y_pred_train_)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd_[:,0], X_test_svd_[:,1], c=y_pred_test_)\n",
    "plt.title('test n_clusters=3, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_test_, y_pred_test_)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rbf\n",
    "sc_rbf_ = SpectralClustering(n_clusters=n_clust_,\n",
    "                            affinity='rbf').fit(X_train_svd_)\n",
    "\n",
    "predict_rbf_train_ = sc_rbf_.fit_predict(X_train_svd_)\n",
    "predict_rbf_test_ = sc_rbf_.fit_predict(X_test_svd_)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=rbf')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd_[:,0], X_train_svd_[:,1], c=predict_rbf_train_)\n",
    "plt.title('train rbf ari: {}'.format(adjusted_rand_score(\n",
    "    y_train_, predict_rbf_train_)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd_[:,0], X_test_svd_[:,1], c=predict_rbf_test_)\n",
    "plt.title('test rbf ari: {}'.format(\n",
    "    adjusted_rand_score(y_test_, predict_rbf_test_)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 10\n",
    "sc_nn10_ = SpectralClustering(n_clusters=n_clust_,\n",
    "                             affinity='nearest_neighbors',\n",
    "                             n_neighbors=10).fit(X_train_svd_)\n",
    "\n",
    "predict_nn10_train_ = sc_nn10_.fit_predict(X_train_svd_)\n",
    "predict_nn10_test_ = sc_nn10_.fit_predict(X_test_svd_)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd_[:,0], X_train_svd_[:,1], c=predict_nn10_train_)\n",
    "plt.title('train nn 10 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train_, predict_nn10_train_)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd_[:,0], X_test_svd_[:,1], c=predict_nn10_test_)\n",
    "plt.title('test nn 10 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test_, predict_nn10_test_)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 50\n",
    "sc_nn50_ = SpectralClustering(n_clusters=n_clust_,\n",
    "                            affinity='nearest_neighbors',\n",
    "                            n_neighbors=50).fit(X_train_svd_)\n",
    "\n",
    "predict_nn50_train_ = sc_nn50_.fit_predict(X_train_svd_)\n",
    "predict_nn50_test_ = sc_nn50_.fit_predict(X_test_svd_)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_svd_[:,0], X_train_svd_[:,1], c=predict_nn50_train_)\n",
    "plt.title('train nn 50 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train_, predict_nn50_train_)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_svd_[:,0], X_test_svd_[:,1], c=predict_nn50_test_)\n",
    "plt.title('test nn 50 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test_, predict_nn50_test_)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results:** perhaps due to the peculiarities of the 3 class data and their overlapping characteristics, these algorithms are much less capable this time around. Especially concerning is the vast difference in ARI scores between train and test sets, where the test set is actually much higher than the train set. Some randomness of the train_test_split for this iteration may have an effect here, even though the results of that do result in equivalent class ratios (as shown in the first section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Classification of 3 classes with supervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_ = KNeighborsClassifier().fit(X_train_tfidf_, y_train_)\n",
    "\n",
    "print('vanilla KNN classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(knn_, X_train_tfidf_, y_train_, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(knn_, X_test_tfidf_, y_test_, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ = LogisticRegression().fit(X_train_tfidf_, y_train_)\n",
    "\n",
    "print('vanilla logistic regression')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(lr_, X_train_tfidf_, y_train_, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(lr_, X_test_tfidf_, y_test_, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_ = RandomForestClassifier().fit(X_train_tfidf_, y_train_)\n",
    "\n",
    "print('vanilla random forest classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(rfc_, X_train_tfidf_, y_train_, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(rfc_, X_test_tfidf_, y_test_, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc_ = xgb.XGBClassifier().fit(X_train_tfidf_, y_train_)\n",
    "\n",
    "print('vanilla xgboost classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(xgbc_, X_train_tfidf_, y_train_, cv=10).mean()))\n",
    "print('test 10 cv mean: {}'.format(cross_val_score(xgbc_, X_test_tfidf_, y_test_, cv=10).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** Again, these classification algorithms are much superior to clustering when it comes to classifying this  data. They do tend to exhibit some overfitting which could be mitigated with some parameter tuning, but even in vanilla form they are much more consistent between train/test sets, which seemed to throw off the clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 UMAP dimension reduction\n",
    "\n",
    "UMAP stands for Uniform Manifold Approximation and Projection, and is useful for visualization and non-linear dimension reduction. This is my first time experimenting with this technique, so all this section will entail is getting some baseline results from UMAP and some light parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "X_train_umap = reducer.fit_transform(X_train_norm)\n",
    "X_test_umap = reducer.fit_transform(X_test_norm)\n",
    "print(X_train_umap.shape)\n",
    "print(X_test_umap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means code\n",
    "kmeans = KMeans(n_clusters=n_clust,\n",
    "                init='k-means++',\n",
    "                n_init=10)\n",
    "\n",
    "y_pred_train = kmeans.fit_predict(X_train_umap)\n",
    "y_pred_test = kmeans.predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('k-means')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_pred_train)\n",
    "plt.title('train n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_train, y_pred_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:, 0], X_test_umap[:, 1], c=y_pred_test)\n",
    "plt.title('test n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_test, y_pred_test)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "#rbf\n",
    "sc_rbf = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='rbf').fit(X_train_umap)\n",
    "\n",
    "predict_rbf_train = sc_rbf.fit_predict(X_train_umap)\n",
    "predict_rbf_test = sc_rbf.fit_predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=rbf')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:,0], X_train_umap[:,1], c=predict_rbf_train)\n",
    "plt.title('train rbf ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_rbf_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:,0], X_test_umap[:,1], c=predict_rbf_test)\n",
    "plt.title('test rbf ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_rbf_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 10 (default)\n",
    "sc_nn10 = SpectralClustering(n_clusters=n_clust,\n",
    "                             affinity='nearest_neighbors',\n",
    "                             n_neighbors=10).fit(X_train_umap)\n",
    "\n",
    "predict_nn10_train = sc_nn10.fit_predict(X_train_umap)\n",
    "predict_nn10_test = sc_nn10.fit_predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('affinity=nearest_neighbors')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:,0], X_train_umap[:,1], c=predict_nn10_train)\n",
    "plt.title('train nn 10 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn10_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:,0], X_test_umap[:,1], c=predict_nn10_test)\n",
    "plt.title('test nn 10 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn10_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 25\n",
    "sc_nn25 = SpectralClustering(n_clusters=n_clust,\n",
    "                             affinity='nearest_neighbors',\n",
    "                             n_neighbors=25).fit(X_train_umap)\n",
    "\n",
    "predict_nn25_train = sc_nn25.fit_predict(X_train_umap)\n",
    "predict_nn25_test = sc_nn25.fit_predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:,0], X_train_umap[:,1], c=predict_nn25_train)\n",
    "plt.title('train nn 25 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn25_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:,0], X_test_umap[:,1], c=predict_nn25_test)\n",
    "plt.title('test nn 25 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn25_test)))\n",
    "plt.show()\n",
    "\n",
    "#nearest neighbors = 5\n",
    "sc_nn5 = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='nearest_neighbors',\n",
    "                            n_neighbors=5).fit(X_train_umap)\n",
    "\n",
    "predict_nn5_train = sc_nn5.fit_predict(X_train_umap)\n",
    "predict_nn5_test = sc_nn5.fit_predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:,0], X_train_umap[:,1], c=predict_nn5_train)\n",
    "plt.title('train nn 5 ari: {}'.format(adjusted_rand_score(\n",
    "    y_train, predict_nn5_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:,0], X_test_umap[:,1], c=predict_nn5_test)\n",
    "plt.title('test nn 5 ari: {}'.format(\n",
    "    adjusted_rand_score(y_test, predict_nn5_test)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** untuned UMAP didn't go well here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP parameter tuning\n",
    "\n",
    "n_neighbors, min_dist: little to no effect on this data\n",
    "metrics tried: euclidean (no effect), manhattan (), cosine (no effect), hamming (error sparse distances), jaccard (sparse distances)\n",
    "\n",
    "While it is something to keep in mind and continue to get familiar with, UMAP dimension reduction for this data is not a very useful technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer_ = umap.UMAP(n_neighbors=20, metric='cosine')\n",
    "X_train_umap_ = reducer_.fit_transform(X_train_norm)\n",
    "X_test_umap_ = reducer_.fit_transform(X_test_norm)\n",
    "print(X_train_umap_.shape)\n",
    "print(X_test_umap_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-means code\n",
    "kmeans = KMeans(n_clusters=n_clust,\n",
    "                init='k-means++',\n",
    "                n_init=10)\n",
    "\n",
    "y_pred_train = kmeans.fit_predict(X_train_umap)\n",
    "y_pred_test = kmeans.predict(X_test_umap)\n",
    "\n",
    "#plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('k-means')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_train_umap[:, 0], X_train_umap[:, 1], c=y_pred_train)\n",
    "plt.title('train n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_train, y_pred_train)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_test_umap[:, 0], X_test_umap[:, 1], c=y_pred_test)\n",
    "plt.title('test n_clusters=2, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y_test, y_pred_test)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
