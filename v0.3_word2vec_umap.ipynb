{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessed from v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bahia cocoa review showers continued throughou...</td>\n",
       "      <td>cocoa</td>\n",
       "      <td>training/1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>computer terminal systems lt cpml completes sa...</td>\n",
       "      <td>acq</td>\n",
       "      <td>training/10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n z trading bank deposit growth rises slightly...</td>\n",
       "      <td>money-supply</td>\n",
       "      <td>training/100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>national amusements again ups viacom lt via bi...</td>\n",
       "      <td>acq</td>\n",
       "      <td>training/1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rogers lt rog sees st qtr net up significantly...</td>\n",
       "      <td>earn</td>\n",
       "      <td>training/10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article      category  \\\n",
       "0  bahia cocoa review showers continued throughou...         cocoa   \n",
       "1  computer terminal systems lt cpml completes sa...           acq   \n",
       "2  n z trading bank deposit growth rises slightly...  money-supply   \n",
       "3  national amusements again ups viacom lt via bi...           acq   \n",
       "4  rogers lt rog sees st qtr net up significantly...          earn   \n",
       "\n",
       "           fileid  \n",
       "0      training/1  \n",
       "1     training/10  \n",
       "2    training/100  \n",
       "3   training/1000  \n",
       "4  training/10000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_df.csv')\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split classes & train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_earn = df.loc[df['category'] == 'earn']\n",
    "df_acq = df.loc[df['category'] == 'acq']\n",
    "df_crude = df.loc[df['category'] == 'crude']\n",
    "df_trade = df.loc[df['category'] == 'trade']\n",
    "df_money = df.loc[df['category'] == 'money-fx']\n",
    "\n",
    "#set data/target for classes\n",
    "X_train_earn, X_test_earn, y_train_earn, y_test_earn = train_test_split(\n",
    "    df_earn['article'], df_earn['category'], test_size=0.25)\n",
    "\n",
    "X_train_acq, X_test_acq, y_train_acq, y_test_acq = train_test_split(\n",
    "    df_acq['article'], df_acq['category'], test_size=0.25)\n",
    "\n",
    "X_train_crude, X_test_crude, y_train_crude, y_test_crude = train_test_split(\n",
    "    df_crude['article'], df_crude['category'], test_size=0.25)\n",
    "\n",
    "X_train_trade, X_test_trade, y_train_trade, y_test_trade = train_test_split(\n",
    "    df_trade['article'], df_trade['category'], test_size=0.25)\n",
    "\n",
    "X_train_money, X_test_money, y_train_money, y_test_money = train_test_split(\n",
    "    df_money['article'], df_money['category'], test_size=0.25)\n",
    "\n",
    "#earn vs acq\n",
    "X_train = pd.concat([X_train_earn, X_train_acq], 0).reset_index(drop=True)\n",
    "X_test = pd.concat([X_test_earn, X_test_acq], 0).reset_index(drop=True)\n",
    "y_train = pd.concat([y_train_earn, y_train_acq], 0).reset_index(drop=True)\n",
    "y_test = pd.concat([y_test_earn, y_test_acq], 0).reset_index(drop=True)\n",
    "\n",
    "#crude vs trade vs money\n",
    "X_train_ = pd.concat([X_train_crude, X_train_trade, X_train_money],\n",
    "                     0).reset_index(drop=True)\n",
    "X_test_ = pd.concat([X_test_crude, X_test_trade, X_test_money],\n",
    "                     0).reset_index(drop=True)\n",
    "y_train_ = pd.concat([y_train_crude, y_train_trade, y_train_money],\n",
    "                     0).reset_index(drop=True)\n",
    "y_test_ = pd.concat([y_test_crude, y_test_trade, y_test_money],\n",
    "                     0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_nlp.to_csv('train_nlp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train own word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp = pd.read_csv('train_nlp_subset.csv')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5515\n"
     ]
    }
   ],
   "source": [
    "bag2 = []\n",
    "for name in train_nlp.columns:\n",
    "    bag2.append(name)\n",
    "\n",
    "#take out category, fileid, and article     \n",
    "bag2 = bag2[3:]\n",
    "print(len(bag2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model2 = word2vec.Word2Vec(bag2,\n",
    "                           workers=2,\n",
    "                           min_count=10,\n",
    "                           window=6,\n",
    "                           sg=0,\n",
    "                           sample=1e-3,\n",
    "                           size=300,\n",
    "                           hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "      <th>fileid</th>\n",
       "      <th>quarter</th>\n",
       "      <th>roger</th>\n",
       "      <th>say</th>\n",
       "      <th>significantly</th>\n",
       "      <th>earning</th>\n",
       "      <th>dlrs</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>reportedly</th>\n",
       "      <th>apollo</th>\n",
       "      <th>pneumo</th>\n",
       "      <th>mailing</th>\n",
       "      <th>fw</th>\n",
       "      <th>brik</th>\n",
       "      <th>tc</th>\n",
       "      <th>max</th>\n",
       "      <th>cental</th>\n",
       "      <th>traf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 5518 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article, category, fileid, quarter, roger, say, significantly, earning, dlrs, year, revenue, sale, lt, see, st, qtr, net, corp, ct, share, company, expect, island, telephone, split, approve, co, ltd, previously, announce, common, shareholder, annual, meeting, vs, profit, mln, shr, loss, revs, inc, nil, note, current, include, charge, discontinue, operation, end, shoe, town, shu, jan, dlr, american, nursery, product, rd, period, feb, mth, avg, shrs, national, th, figure, pro, forma, rev, discus, disc, seven, entertainment, publication, unit, disposal, pay, n, pak, store, pnp, s, volvo, billion, slightly, report, earlier, crown, industrial, high, result, ab, exclude, goldfield, gv, discontinued, federated, department, up, qtly, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 5518 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.DataFrame(columns=bag2)\n",
    "vocab = model2.wv.vocab.keys()\n",
    "\n",
    "for i, word in enumerate(X_df)\n",
    "X_df[:3]\n",
    "#for word in bag2:\n",
    "#    if word not in X_df.columns and word in vocab:\n",
    "#            X_df[word] = ''\n",
    "#            X_df.loc[i, word] = [model2.wv[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, article in enumerate(train_nlp_subset['article']):\n",
    "    #parse article\n",
    "    article_nlp = nlp(article)\n",
    "    #filter stopwords, punctuation\n",
    "    article = [token.lemma_ for token in article_nlp \n",
    "                if not token.is_punct and not token.is_stop]\n",
    "    #bag 20 most common words\n",
    "    bag = ([item[0] for item in Counter(article).most_common(20)])\n",
    "    #add new words as features and populate rows with word vector\n",
    "    #df_temp = pd.DataFrame()\n",
    "    for word in bag:\n",
    "        if word not in train_nlp.columns and word in model.wv.vocab.keys():\n",
    "            train_nlp[word] = ''\n",
    "            train_nlp.loc[i, word] = np.mean([model.wv[word]])\n",
    "train_nlp.replace('', np.mean(np.zeros(300,)), inplace=True)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6215, 5515) (6215,)\n"
     ]
    }
   ],
   "source": [
    "X = train_nlp.loc[:, ~train_nlp.columns.isin(['article', 'category', 'fileid'])]\n",
    "y = train_nlp['category']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "X_norm = normalize(X)\n",
    "X_svd = TruncatedSVD(2).fit_transform(X_norm)\n",
    "\n",
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "X_umap = reducer.fit_transform(X_norm)\n",
    "\n",
    "n_clust = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gt = pd.Categorical(y).codes\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_svd[:,0], X_svd[:,1], c=y_gt)\n",
    "plt.title('ground truth svd')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_umap[:,0], X_umap[:,1], c=y_gt)\n",
    "plt.title('ground truth umap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clust,\n",
    "                init='k-means++',\n",
    "                n_init=10)\n",
    "\n",
    "y_pred_svd = kmeans.fit_predict(X_svd)\n",
    "y_pred_umap = kmeans.fit_predict(X_umap)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.suptitle('k-means')\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_svd[:, 0], X_svd[:, 1], c=y_pred_svd)\n",
    "plt.title('svd, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y, y_pred_svd)))\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y_pred_umap)\n",
    "plt.title('umap, ari={:0.5}'.format(\n",
    "    adjusted_rand_score(y, y_pred_umap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "xgbc = xgb.XGBClassifier().fit(X, y)\n",
    "\n",
    "print('vanilla xgboost classifier')\n",
    "print('train 10 cv mean: {}'.format(cross_val_score(xgbc, X, y, cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "#rbf\n",
    "sc_rbf = SpectralClustering(n_clusters=n_clust,\n",
    "                            affinity='rbf').fit(X_svd)\n",
    "\n",
    "predict_rbf_train = sc_rbf.fit_predict(X_svd)\n",
    "#plots\n",
    "plt.figure(figsize=(6,6))\n",
    "#plt.suptitle('affinity=rbf')\n",
    "#plt.subplot(121)\n",
    "plt.scatter(X_svd[:,0], X_svd[:,1], c=predict_rbf_train)\n",
    "plt.title('train rbf ari: {}'.format(adjusted_rand_score(\n",
    "    y, predict_rbf_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag2 = []\n",
    "for name in train_nlp.columns:\n",
    "    bag2.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error stop here\n",
    "#zeros = np.zeros([300,], dtype=list)\n",
    "start_time = time.time()\n",
    "\n",
    "#train_nlp = pd.concat([df_earn, df_acq], 0).reset_index(drop=True)\n",
    "\n",
    "for i, article in enumerate(train_nlp_subset['article']):\n",
    "    #parse article\n",
    "    article_nlp = nlp(article)\n",
    "    #filter stopwords, punctuation\n",
    "    article = [token.lemma_ for token in article_nlp \n",
    "                if not token.is_punct and not token.is_stop]\n",
    "    #bag 20 most common words\n",
    "    bag = ([item[0] for item in Counter(article).most_common(20)])\n",
    "    #add new words as features and populate rows with word vector\n",
    "    #df_temp = pd.DataFrame()\n",
    "    for word in bag:\n",
    "        if word not in train_nlp.columns and word in model.wv.vocab.keys():\n",
    "            train_nlp[word] = ''\n",
    "            train_nlp.loc[i, word] = np.mean([model.wv[word]])\n",
    "train_nlp.replace('', np.mean(np.zeros(300,)), inplace=True)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spacey parsing, word2vec token vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nlp = pd.read_csv('train_nlp_subset.csv')\n",
    "#train_nlp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#start_time = time.time()\n",
    "#parser\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#load pre trained google model\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "#    './GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#print(time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
