{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Capstone:\n",
    "\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/brianmcguckin/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters, stopwords\n",
    "#print(reuters.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769\n",
      "3019\n"
     ]
    }
   ],
   "source": [
    "#each file is pre-labeled for train (~73%) and test (remaining 27%)\n",
    "train_files = list(filter(lambda x: x.startswith('training'), reuters.fileids()))\n",
    "test_files = list(filter(lambda x: x.startswith('test'), reuters.fileids()))\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "train_cats = reuters.categories(train_files)\n",
    "test_cats = reuters.categories(test_files)\n",
    "\n",
    "#does the test set contain any categories that the train set doesn't?\n",
    "print([x for x in test_cats if x not in train_cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating th\n"
     ]
    }
   ],
   "source": [
    "#see what this data looks like\n",
    "train_raw = reuters.raw(train_files)\n",
    "test_raw = reuters.raw(test_files)\n",
    "\n",
    "print(train_raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training/1',\n",
       " 'training/10',\n",
       " 'training/100',\n",
       " 'training/1000',\n",
       " 'training/10000']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAHIA COCOA REVIEW\n",
      "  Showers continued throughout the week in\n",
      "  the Bahia cocoa zone, alleviating th\n"
     ]
    }
   ],
   "source": [
    "asasdf = reuters.raw('training/1')\n",
    "print(asasdf[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1253696\n",
      "6478471\n",
      "467205\n",
      "2368382\n"
     ]
    }
   ],
   "source": [
    "print(len(reuters.words(train_files)))\n",
    "print(len(reuters.raw(train_files)))\n",
    "print(len(reuters.words(test_files)))\n",
    "print(len(reuters.raw(test_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7769 (7769, 4)\n",
      "3019 (3019, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['fileid', 'raw', 'words', 'category'])\n",
    "for file in train_files:\n",
    "    train_df = train_df.append({\n",
    "        'fileid':file,\n",
    "        'raw':reuters.raw([file]),\n",
    "        'words':reuters.words(file),\n",
    "        'category':reuters.categories(file)}, ignore_index=True)\n",
    "print(len(train_files), train_df.shape)\n",
    "\n",
    "test_df = pd.DataFrame(columns=['fileid', 'raw', 'words', 'category'])\n",
    "for file in test_files:\n",
    "    test_df = test_df.append({\n",
    "        'fileid':file,\n",
    "        'raw':reuters.raw(file),\n",
    "        'words':reuters.words(file),\n",
    "        'category':reuters.categories(file)}, ignore_index=True)\n",
    "print(len(test_files), test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fileid</th>\n",
       "      <th>raw</th>\n",
       "      <th>words</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "      <td>(ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U, ., S...</td>\n",
       "      <td>[trade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "      <td>(CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...</td>\n",
       "      <td>[grain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "      <td>(JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "      <td>(THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "      <td>(INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test/14839</td>\n",
       "      <td>AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS...</td>\n",
       "      <td>(AUSTRALIAN, FOREIGN, SHIP, BAN, ENDS, BUT, NS...</td>\n",
       "      <td>[ship]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test/14840</td>\n",
       "      <td>INDONESIAN COMMODITY EXCHANGE MAY EXPAND\\n  Th...</td>\n",
       "      <td>(INDONESIAN, COMMODITY, EXCHANGE, MAY, EXPAND,...</td>\n",
       "      <td>[coffee, lumber, palm-oil, rubber, veg-oil]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test/14841</td>\n",
       "      <td>SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n...</td>\n",
       "      <td>(SRI, LANKA, GETS, USDA, APPROVAL, FOR, WHEAT,...</td>\n",
       "      <td>[grain, wheat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test/14842</td>\n",
       "      <td>WESTERN MINING TO OPEN NEW GOLD MINE IN AUSTRA...</td>\n",
       "      <td>(WESTERN, MINING, TO, OPEN, NEW, GOLD, MINE, I...</td>\n",
       "      <td>[gold]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test/14843</td>\n",
       "      <td>SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERG...</td>\n",
       "      <td>(SUMITOMO, BANK, AIMS, AT, QUICK, RECOVERY, FR...</td>\n",
       "      <td>[acq]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fileid                                                raw  \\\n",
       "0  test/14826  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...   \n",
       "1  test/14828  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...   \n",
       "2  test/14829  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...   \n",
       "3  test/14832  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...   \n",
       "4  test/14833  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...   \n",
       "5  test/14839  AUSTRALIAN FOREIGN SHIP BAN ENDS BUT NSW PORTS...   \n",
       "6  test/14840  INDONESIAN COMMODITY EXCHANGE MAY EXPAND\\n  Th...   \n",
       "7  test/14841  SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n...   \n",
       "8  test/14842  WESTERN MINING TO OPEN NEW GOLD MINE IN AUSTRA...   \n",
       "9  test/14843  SUMITOMO BANK AIMS AT QUICK RECOVERY FROM MERG...   \n",
       "\n",
       "                                               words  \\\n",
       "0  (ASIAN, EXPORTERS, FEAR, DAMAGE, FROM, U, ., S...   \n",
       "1  (CHINA, DAILY, SAYS, VERMIN, EAT, 7, -, 12, PC...   \n",
       "2  (JAPAN, TO, REVISE, LONG, -, TERM, ENERGY, DEM...   \n",
       "3  (THAI, TRADE, DEFICIT, WIDENS, IN, FIRST, QUAR...   \n",
       "4  (INDONESIA, SEES, CPO, PRICE, RISING, SHARPLY,...   \n",
       "5  (AUSTRALIAN, FOREIGN, SHIP, BAN, ENDS, BUT, NS...   \n",
       "6  (INDONESIAN, COMMODITY, EXCHANGE, MAY, EXPAND,...   \n",
       "7  (SRI, LANKA, GETS, USDA, APPROVAL, FOR, WHEAT,...   \n",
       "8  (WESTERN, MINING, TO, OPEN, NEW, GOLD, MINE, I...   \n",
       "9  (SUMITOMO, BANK, AIMS, AT, QUICK, RECOVERY, FR...   \n",
       "\n",
       "                                         category  \n",
       "0                                         [trade]  \n",
       "1                                         [grain]  \n",
       "2                                [crude, nat-gas]  \n",
       "3  [corn, grain, rice, rubber, sugar, tin, trade]  \n",
       "4                             [palm-oil, veg-oil]  \n",
       "5                                          [ship]  \n",
       "6     [coffee, lumber, palm-oil, rubber, veg-oil]  \n",
       "7                                  [grain, wheat]  \n",
       "8                                          [gold]  \n",
       "9                                           [acq]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6064562\n",
      "2197712\n"
     ]
    }
   ],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub('[\\[].*?[\\]]','',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "train = text_cleaner(train_raw)\n",
    "test = text_cleaner(test_raw)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conform with spacys character limit\n",
    "train_substrings = [train[i:i+999999] for i in range(0, len(train), 999999)]\n",
    "test_substrings = [test[i:i+999999] for i in range(0, len(test), 999999)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = train_substrings[0]\n",
    "train1 = train_substrings[1]\n",
    "train2 = train_substrings[2]\n",
    "train3 = train_substrings[3]\n",
    "train4 = train_substrings[4]\n",
    "train5 = train_substrings[5]\n",
    "train6 = train_substrings[6]\n",
    "\n",
    "test0 = test_substrings[0]\n",
    "test1 = test_substrings[1]\n",
    "test2 = test_substrings[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "start = time.clock()\n",
    "train0_doc = nlp(train0)\n",
    "print(time.clock() - start)\n",
    "train1_doc = nlp(train1)\n",
    "print(time.clock() - start)\n",
    "train2_doc = nlp(train2)\n",
    "print(time.clock() - start)\n",
    "train3_doc = nlp(train3)\n",
    "print(time.clock() - start)\n",
    "train4_doc = nlp(train4)\n",
    "print(time.clock() - start)\n",
    "train5_doc = nlp(train5)\n",
    "print(time.clock() - start)\n",
    "train6_doc = nlp(train6)\n",
    "print(time.clock() - start)\n",
    "test0_doc = nlp(test0)\n",
    "print(time.clock() - start)\n",
    "test1_doc = nlp(test1)\n",
    "print(time.clock() - start)\n",
    "test2_doc = nlp(test2)\n",
    "print(time.clock() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [train0_doc, train1_doc, train2_doc, train3_doc, train4_doc, train5_doc, train6_doc]\n",
    "train_tokens = []\n",
    "for doc in train_docs:\n",
    "    for token in doc:\n",
    "        train_tokens.append(token)\n",
    "        \n",
    "test_docs = [test0_doc, test1_doc, test2_doc]\n",
    "test_tokens = []\n",
    "for doc in test_docs:\n",
    "    for token in doc:\n",
    "        test_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure this worked\n",
    "print(len(train_tokens))\n",
    "print(len(train0_doc) + len(train1_doc) + len(train2_doc) + len(train3_doc)\n",
    "      + len(train4_doc) + len(train5_doc) + len(train6_doc))\n",
    "\n",
    "print(len(test_tokens))\n",
    "print(len(test0_doc) + len(test1_doc) + len(test2_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
